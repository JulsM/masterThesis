{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load raceTimePredictor.py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn import preprocessing\n",
    "import math as math\n",
    "import os, shutil\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "\n",
    "class RaceTimePredictor:\n",
    "\t\n",
    "\tdef __init__(self, FLAGS={}):\n",
    "\n",
    "\n",
    "\t\tself.FLAGS = {'learning_rate': 0.01,\n",
    "\t\t\t\t\t\t'training_steps': 40000,\n",
    "\t\t\t\t\t\t'batch_size': 128,\n",
    "\t\t\t\t\t\t'model_path': \"tf_checkpoints/\",\n",
    "\t\t\t\t\t\t'data_path' : 'kmeans'}\n",
    "\t\tfor key in FLAGS:\n",
    "\t\t\tself.FLAGS[key] = FLAGS[key]\n",
    "\n",
    "\t\tself.pathDict = {'races' : \"../output/raceFeatures.csv\",\n",
    "\t\t\t\t\t'kmeans': \"../output/trainFeatures.csv\",\n",
    "\t\t\t\t\t'all': \"../output/activitiesFeatures.csv\",\n",
    "\t\t\t\t\t'set' : \"../output/activitySetFeatures.csv\",\n",
    "\t\t\t\t\t'pred' : \"../output/predictions.csv\"}\n",
    "\t\t\n",
    "\t\tself.FEATURE_PATH = self.pathDict[self.FLAGS['data_path']]\n",
    "\t\tself.PRED_PATH = self.pathDict['pred']\n",
    "\t\tself.COLUMNS = [\"dist\", \"elev\", \"hilly\", \"cs\", \"atl\", \"ctl\", \"isRace\", \"avgVo2max\", \"time\", \"avgTrainPace\"]\n",
    "\t\tself.FEATURES = [\"dist\", \"elev\", \"hilly\", \"cs\", \"atl\", \"ctl\", \"isRace\", \"avgVo2max\", \"avgTrainPace\"]\n",
    "\n",
    "\t\tself.LABEL = \"time\"\n",
    "\n",
    "\t\tself.training_set = None\n",
    "\t\tself.test_set = None\n",
    "\n",
    "\n",
    "\t\t\n",
    "\n",
    "\t\t\n",
    "\n",
    "\t\n",
    "\n",
    "\tdef plotStatistics(self):\n",
    "\t\ttraining_set, test_set, prediction_set = loadData()\n",
    "\t\t# training_set, test_set, prediction_set = normalize(training_set, test_set, prediction_set)\n",
    "\t\tdataset = pd.concat([training_set, test_set])\n",
    "\t\t# training_set.hist()\n",
    "\n",
    "\t\t# dataset.plot(kind='density', subplots=True, layout=(3,3), sharex=False)\n",
    "\n",
    "\t\t# pd.plotting.scatter_matrix(dataset)\n",
    "\n",
    "\t\tcax = plt.matshow(dataset.corr(), vmin=-1, vmax=1)\n",
    "\t\tplt.colorbar(cax)\n",
    "\t\tlocs, labs = plt.xticks()\n",
    "\t\tplt.xticks(np.arange(len(COLUMNS)), COLUMNS)\n",
    "\t\tplt.yticks(np.arange(len(COLUMNS)), COLUMNS)\n",
    "\n",
    "\t\t# plt.scatter(dataset['dist'], dataset['CS'])\n",
    "\t\tplt.show()\n",
    "\n",
    "\tdef clearOldFiles(self):\n",
    "\t\tfilelist = [ f for f in os.listdir(self.FLAGS['model_path'])]\n",
    "\t\tfor f in filelist:\n",
    "\t\t\t# os.chmod(os.path.join(self.FLAGS['model_path'], f), 777)\n",
    "\t\t\tshutil.rmtree(os.path.join(self.FLAGS['model_path'], f))\n",
    "\t\t# if tf.gfile.Exists(self.FLAGS['model_path']):\n",
    "\t #   \t\ttf.gfile.DeleteRecursively(self.FLAGS['model_path']) \n",
    "\n",
    "\tdef normalize(self, data):\n",
    "\t\t# mean, std = train[FEATURES].mean(axis=0), train[FEATURES].std(axis=0, ddof=0)\n",
    "\t\t\n",
    "\t\tdata[self.FEATURES] = self.std_scaler.transform(data[self.FEATURES])\n",
    "\t\t# print(data)\n",
    "\t\treturn data\n",
    "\n",
    "\n",
    "\t\n",
    "\tdef loadTrainData(self):\n",
    "\t\ttrain_data = pd.read_csv(self.FEATURE_PATH, skipinitialspace=True, skiprows=1, names=self.COLUMNS)\n",
    "\t\ttrain_data = pd.DataFrame(train_data, columns=self.COLUMNS)\n",
    "\n",
    "\t\treturn train_data\n",
    "\n",
    "\tdef splitKFold(self, k, i):\n",
    "\t\t# print(self.train_data)\n",
    "\t\traces = self.train_data[self.train_data.isRace == 1]\n",
    "\t\tnoRaces = pd.DataFrame(self.train_data[self.train_data.isRace == -1], columns=self.COLUMNS)\n",
    "\t\tfoldLen = int(len(races) / k)\n",
    "\t\tstart = i * foldLen\n",
    "\t\tif i + 1 == k:\n",
    "\t\t\tend = len(races)\n",
    "\t\telse:\n",
    "\t\t\tend = (i + 1) * foldLen\n",
    "\t\t# print('len races: ', len(races))\n",
    "\t\t# print(start, end)\n",
    "\t\t# print(races)\n",
    "\t\ttest = races[start:end]\n",
    "\t\ttrain = races[0:start]\n",
    "\t\ttrain = train.append(races[end:])\n",
    "\t\t\n",
    "\t\tself.test_set = pd.DataFrame(test, columns=self.COLUMNS).reset_index(drop=True)\n",
    "\t\tself.training_set = noRaces.append(train).reset_index(drop=True)\n",
    "\t\t# print(self.training_set, self.test_set)\n",
    "\t\tprint('train size: ',len(self.training_set), ' test size: ', len(self.test_set))\n",
    "\n",
    "\n",
    "\n",
    "\tdef get_input_fn(self, data_set, num_epochs=None, shuffle=True):\n",
    "\t\t\treturn tf.estimator.inputs.pandas_input_fn(x=pd.DataFrame({k: data_set[k].values for k in self.FEATURES}), \n",
    "\t\t  \t\ty = pd.Series(data_set[self.LABEL].values), batch_size=self.FLAGS['batch_size'], num_epochs=num_epochs, shuffle=shuffle)\n",
    "\n",
    "\n",
    "\n",
    "\tdef model_fn(self, features, labels, mode, params):\n",
    "\t \n",
    "\n",
    "\t\t# Connect the first hidden layer to input layer\n",
    "\t\tfeature_cols = [tf.feature_column.numeric_column(k) for k in self.FEATURES]\n",
    "\t\tinput_layer = tf.feature_column.input_layer(features=features, feature_columns=feature_cols)\n",
    "\n",
    "\n",
    "\t\t# Connect the first hidden layer to second hidden layer with relu\n",
    "\t\thidden_layer = tf.layers.dense(input_layer, 10, activation=tf.nn.relu, \n",
    "\t\t\tkernel_regularizer=tf.contrib.layers.l1_l2_regularizer(scale_l1=1.0, scale_l2=1.0), name='hidden_1')\n",
    "\n",
    "\t\th1_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'hidden_1')\n",
    "\t\ttf.summary.histogram('kernel_1', h1_vars[0])\n",
    "\t\ttf.summary.histogram('bias_1', h1_vars[1])\n",
    "\t\ttf.summary.histogram('activation_1', hidden_layer)\n",
    "\n",
    "\t\tif mode == tf.estimator.ModeKeys.TRAIN:\n",
    "\t\t\thidden_layer = tf.layers.dropout(hidden_layer, rate=0.3, name='dropout_1')\n",
    "\t\t\ttf.summary.scalar('dropout_1', tf.nn.zero_fraction(hidden_layer))\n",
    "\n",
    "\n",
    "\n",
    "\t\t# Connect the second hidden layer to first hidden layer with relu\n",
    "\t\thidden_layer = tf.layers.dense(hidden_layer, 10, activation=tf.nn.relu, \n",
    "\t\t\tkernel_regularizer=tf.contrib.layers.l1_l2_regularizer(scale_l1=1.0, scale_l2=1.0), name='hidden_2')\n",
    "\n",
    "\t\th2_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'hidden_2')\n",
    "\t\ttf.summary.histogram('kernel_2', h2_vars[0])\n",
    "\t\ttf.summary.histogram('bias_2', h2_vars[1])\n",
    "\t\ttf.summary.histogram('activation_2', hidden_layer)\n",
    "\n",
    "\t\tif mode == tf.estimator.ModeKeys.TRAIN:\n",
    "\t\t\thidden_layer = tf.layers.dropout(hidden_layer, rate=0.3, name='dropout_2')\n",
    "\t\t\ttf.summary.scalar('dropout_2', tf.nn.zero_fraction(hidden_layer))\n",
    "\n",
    "\n",
    "\n",
    "\t\t# Connect the output layer to second hidden layer (no activation fn)\n",
    "\t\toutput_layer = tf.layers.dense(hidden_layer, 1, name='output')\n",
    "\n",
    "\t\t# Reshape output layer to 1-dim Tensor to return predictions\n",
    "\t\tpredictions = tf.reshape(output_layer, [-1])\n",
    "\n",
    "\t\t# Provide an estimator spec for `ModeKeys.PREDICT`.\n",
    "\t\tif mode == tf.estimator.ModeKeys.PREDICT:\n",
    "\t\t\treturn tf.estimator.EstimatorSpec(mode=mode,predictions={self.LABEL: predictions})\n",
    "\n",
    "\n",
    "\t\t# Calculate loss using mean squared error\n",
    "\t\tloss = tf.losses.mean_squared_error(labels, predictions)\n",
    "\n",
    "\t\treg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "\t\tloss = tf.add_n([loss] + reg_losses)\n",
    "\n",
    "\t\t\n",
    "\t\ttf.summary.scalar(\"reg_loss\", reg_losses[0])\n",
    "\t\ttf.summary.scalar(\"train_error\", loss)\n",
    "\n",
    "\n",
    "\n",
    "\t\toptimizer = tf.train.AdamOptimizer(learning_rate=params[\"learning_rate\"])\n",
    "\t\ttrain_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n",
    "\n",
    "\t\talpha_t = optimizer._lr * tf.sqrt(1-optimizer._beta2_power) / (1-optimizer._beta1_power)\n",
    "\t\ttf.summary.scalar(\"learning_rate\", alpha_t)\n",
    "\t\t\n",
    "\t\t\n",
    "\n",
    "\t\t# Calculate root mean squared error as additional eval metric\n",
    "\t\teval_metric_ops = {\n",
    "\t\t  \"rmse\": tf.metrics.root_mean_squared_error(tf.cast(labels, tf.float64), tf.cast(predictions, tf.float64)),\n",
    "\t\t  \"r\" : tf.contrib.metrics.streaming_pearson_correlation(tf.cast(predictions, tf.float32), tf.cast(labels, tf.float32))\n",
    "\t\t}\n",
    "\t\t\n",
    "\t\t# Provide an estimator spec for `ModeKeys.EVAL` and `ModeKeys.TRAIN` modes.\n",
    "\t\treturn tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op, eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "\n",
    "\t\treturn EstimatorSpec(mode, predictions, loss, train_op, eval_metric_ops)\n",
    "\n",
    "\n",
    "\tdef trainPredictor(self):\n",
    "\t\ttrain_input_fn = self.get_input_fn(self.training_set, num_epochs=None, shuffle=True)\n",
    "\n",
    "\t\t# Train\n",
    "\t\tself.estimator.train(input_fn=train_input_fn, steps=self.FLAGS['training_steps'])\n",
    "\n",
    "\n",
    "\tdef evaluatePredictor(self):\n",
    "\t\t# Score accuracy\n",
    "\t\ttest_input_fn = self.get_input_fn(self.test_set, num_epochs=1, shuffle=False)\n",
    "\t\tev = self.estimator.evaluate(input_fn=test_input_fn)\n",
    "\t\tprint(\"Loss: %s\" % ev['loss'])\n",
    "\t\tprint(\"Root Mean Squared Error: %s\\n\" % ev[\"rmse\"])\n",
    "\t\t# print(\"R: %s\" % ev[\"r\"])\n",
    "\t\treturn ev\n",
    "\n",
    "\n",
    "\tdef predictTimes(self):\n",
    "\t\tprint('\\nPredictions\\n')\n",
    "\t\tpred_data = pd.read_csv(self.PRED_PATH, skipinitialspace=True, skiprows=1, names=self.COLUMNS)\n",
    "\t\tprediction_set = pd.DataFrame(pred_data, columns=self.COLUMNS)\n",
    "\t\tprint(prediction_set)\n",
    "\t\tprediction_set = self.normalize(prediction_set)\n",
    "\t\t# prediction_set = pd.DataFrame([(21000, 400, 0.5, 1.8, 44, 45, 1, 43, 90,3.448)], columns=COLUMNS)\n",
    "\t\t# Print out predictions\n",
    "\n",
    "\t\tpredict_input_fn = self.get_input_fn(prediction_set, num_epochs=1, shuffle=False)\n",
    "\t\tpredictions = self.estimator.predict(input_fn=predict_input_fn)\n",
    "\t\tpred = list()\n",
    "\t\t\n",
    "\t\tfor i, p in enumerate(predictions):\n",
    "\t\t\tprint(\"Predicted time %s: %s\" % (i, round(p[self.LABEL], 2)))\n",
    "\t\t\tpred.append(p[self.LABEL])\n",
    "\t\t\tprint(\"+/- Seconds: %s\" % (round((p[self.LABEL] - prediction_set[self.LABEL][i]) * 60, 2)))\n",
    "\n",
    "\t\n",
    "\n",
    "\n",
    "\tdef trainCrossValidated(self, kfold):\n",
    "\t\tprint('Load Data\\n')\n",
    "\t\tself.train_data = self.loadTrainData()\n",
    "\t\tself.clearOldFiles()\n",
    "\t\tkfoldLosses = []\n",
    "\t\tkfoldRmse = []\n",
    "\t\tprint('Start training\\n')\n",
    "\t\tfor i in range(kfold):\n",
    "\t\t\tmodel_params = {\"learning_rate\": self.FLAGS['learning_rate']}\n",
    "\t\t\tself.estimator = tf.estimator.Estimator(model_fn=self.model_fn, params=model_params, model_dir=self.FLAGS['model_path']+'temp_'+str(i))\n",
    "\n",
    "\t\t\tself.splitKFold(kfold, i)\n",
    "\t\t\tself.std_scaler = preprocessing.StandardScaler().fit(self.training_set[self.FEATURES])\n",
    "\t\t\tself.training_set = self.normalize(self.training_set)\n",
    "\t\t\tself.test_set = self.normalize(self.test_set)\n",
    "\t\t\t\n",
    "\t\t\tself.trainPredictor()\n",
    "\t\t\tmetrics = self.evaluatePredictor()\n",
    "\t\t\tkfoldLosses.append(metrics['loss'])\n",
    "\t\t\tkfoldRmse.append(metrics['rmse'])\n",
    "\n",
    "\t\tprint(\"\\nMean loss for %s-fold cross validation: %s\" % (kfold, np.mean(kfoldLosses)))\n",
    "\t\tprint(\"Mean RMSE for %s-fold cross validation: %s\" % (kfold, np.mean(kfoldRmse)))\n",
    "\t\tself.predictTimes()\n",
    "\n",
    "\n",
    "\tdef trainStandard(self):\n",
    "\t\tprint('Load Data\\n')\n",
    "\t\tself.training_set = self.loadTrainData()\n",
    "\t\tself.clearOldFiles()\n",
    "\n",
    "\t\tmodel_params = {\"learning_rate\": self.FLAGS['learning_rate']}\t\t\n",
    "\t\tself.estimator = tf.estimator.Estimator(model_fn=self.model_fn, params=model_params, model_dir=self.FLAGS['model_path']+'temp')\n",
    "\n",
    "\t\tself.std_scaler = preprocessing.StandardScaler().fit(self.training_set[self.FEATURES])\n",
    "\t\tself.training_set = self.normalize(self.training_set)\n",
    "\n",
    "\t\ttest_data = pd.read_csv(self.PRED_PATH, skipinitialspace=True, skiprows=1, names=self.COLUMNS)\n",
    "\t\ttest_set = pd.DataFrame(test_data, columns=self.COLUMNS)\n",
    "\t\tself.test_set = self.normalize(test_set)\n",
    "\t\tprint('train size: ',len(self.training_set), ' test size: ', len(self.test_set))\n",
    "\n",
    "\t\tprint('Start training\\n')\n",
    "\t\tself.trainPredictor()\n",
    "\n",
    "\t\tself.evaluatePredictor()\n",
    "\n",
    "\t\tself.predictTimes()\n",
    "\n",
    "\tdef trainWithPretraining(self, kfold):\n",
    "\t\tprint('Pre-train Set\\n')\n",
    "\t\tself.FEATURE_PATH = self.pathDict['set']\n",
    "\t\tself.training_set = self.loadTrainData()\n",
    "\t\tprint('train size: ',len(self.training_set))\n",
    "\t\tself.clearOldFiles()\n",
    "\n",
    "\t\tmodel_params = {\"learning_rate\": self.FLAGS['learning_rate']}\n",
    "\t\tself.estimator = tf.estimator.Estimator(model_fn=self.model_fn, params=model_params, model_dir=self.FLAGS['model_path']+'pretrain')\n",
    "\t\tself.std_scaler = preprocessing.StandardScaler().fit(self.training_set[self.FEATURES])\n",
    "\t\tself.training_set = self.normalize(self.training_set)\n",
    "\t\tprint('Start training\\n')\n",
    "\t\tself.trainPredictor()\n",
    "\n",
    "\t\tprint('Pre-train kmeans\\n')\n",
    "\t\tself.FEATURE_PATH = self.pathDict['kmeans']\n",
    "\t\tself.training_set = self.loadTrainData()\n",
    "\t\tprint('train size: ',len(self.training_set))\n",
    "\t\tself.std_scaler = preprocessing.StandardScaler().fit(self.training_set[self.FEATURES])\n",
    "\t\tself.training_set = self.normalize(self.training_set)\n",
    "\t\tprint('Start training\\n')\n",
    "\t\tself.trainPredictor()\n",
    "\n",
    "\t\t### copy pre-trained model to kfold folders\n",
    "\t\tfor i in range(kfold):\n",
    "\t\t\tsrc = self.FLAGS['model_path']+'pretrain'\n",
    "\t\t\tdst = self.FLAGS['model_path']+'temp_'+str(i)\n",
    "\t\t\tshutil.copytree(src, dst)\n",
    "\n",
    "\t\t\n",
    "\t\tprint('Train races\\n')\n",
    "\t\tself.FEATURE_PATH = self.pathDict['races']\n",
    "\t\tself.train_data = self.loadTrainData()\n",
    "\t\tkfoldLosses = []\n",
    "\t\tkfoldRmse = []\n",
    "\t\tprint('Start training\\n')\n",
    "\t\tfor i in range(kfold):\n",
    "\t\t\tmodel_params = {\"learning_rate\": self.FLAGS['learning_rate']}\n",
    "\t\t\tself.estimator = tf.estimator.Estimator(model_fn=self.model_fn, params=model_params, model_dir=self.FLAGS['model_path']+'temp_'+str(i))\n",
    "\n",
    "\t\t\tself.splitKFold(kfold, i)\n",
    "\t\t\tself.std_scaler = preprocessing.StandardScaler().fit(self.training_set[self.FEATURES])\n",
    "\t\t\tself.training_set = self.normalize(self.training_set)\n",
    "\t\t\tself.test_set = self.normalize(self.test_set)\n",
    "\t\t\t\n",
    "\t\t\tself.trainPredictor()\n",
    "\t\t\tmetrics = self.evaluatePredictor()\n",
    "\t\t\tkfoldLosses.append(metrics['loss'])\n",
    "\t\t\tkfoldRmse.append(metrics['rmse'])\n",
    "\n",
    "\t\tprint(\"\\nMean loss for %s-fold cross validation: %s\" % (kfold, np.mean(kfoldLosses)))\n",
    "\t\tprint(\"Mean RMSE for %s-fold cross validation: %s\" % (kfold, np.mean(kfoldRmse)))\n",
    "\t\tself.predictTimes()\n",
    "\t\t\n",
    "def main(unused_argv):\n",
    "\tpredictor = RaceTimePredictor({'training_steps': 40000, 'data_path' : 'all'})\n",
    "\tpredictor.trainWithPretraining(4)\n",
    "\t# predictor.trainCrossValidated(4)\n",
    "\t# predictor.trainStandard()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\ttf.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-training: Set, All, Races. Own races in Set and All excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
